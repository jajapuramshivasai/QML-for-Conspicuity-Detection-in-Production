{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cd5948",
   "metadata": {},
   "outputs": [],
   "source": [
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f7d302",
   "metadata": {},
   "source": [
    "### Variational Quantum Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530bdda4",
   "metadata": {},
   "source": [
    "VQC is a parameterized model consisting of quantum gates and entanglement operations. These circuits are designed to encode data into quantum states and then perform operations that can reveal patterns useful for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8939df64-457e-4fbb-afe9-9a6d2cba35a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Cost = 0.9003227437604365\n",
      "Epoch 10: Cost = 0.8319896373828645\n",
      "Epoch 15: Cost = 0.7978646128268697\n",
      "Epoch 20: Cost = 0.7810886638506145\n",
      "Epoch 25: Cost = 0.7688536308257489\n",
      "Epoch 30: Cost = 0.7575372167375625\n",
      "Epoch 35: Cost = 0.7446157923528166\n",
      "Epoch 40: Cost = 0.735726071392138\n",
      "Epoch 45: Cost = 0.7301319706020605\n",
      "Epoch 50: Cost = 0.7255258808790681\n",
      "Epoch 55: Cost = 0.721812099820872\n",
      "Epoch 60: Cost = 0.7191745756759761\n",
      "Epoch 65: Cost = 0.716785177703814\n",
      "Epoch 70: Cost = 0.7142318615452196\n",
      "Epoch 75: Cost = 0.711690429107643\n",
      "Epoch 80: Cost = 0.709382512513614\n",
      "Epoch 85: Cost = 0.707348403272641\n",
      "Epoch 90: Cost = 0.7054600053397403\n",
      "Epoch 95: Cost = 0.7036429688732763\n",
      "Epoch 100: Cost = 0.7019948650964521\n",
      "Epoch 105: Cost = 0.7006778881227044\n",
      "Epoch 110: Cost = 0.6997518130000672\n",
      "Epoch 115: Cost = 0.6991336877670099\n",
      "Epoch 120: Cost = 0.6986992165324531\n",
      "Epoch 125: Cost = 0.698367090278164\n",
      "Epoch 130: Cost = 0.6980970032775823\n",
      "Epoch 135: Cost = 0.6978667678746505\n",
      "Epoch 140: Cost = 0.6976631132736939\n",
      "Epoch 145: Cost = 0.697477925748824\n",
      "Epoch 150: Cost = 0.6973055838129887\n",
      "Test Accuracy: 81.58%\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "import pandas as pd\n",
    "from pennylane.optimize import NesterovMomentumOptimizer\n",
    "import math\n",
    "\n",
    "'''Data preparation: This involves data loading, split, encoding \n",
    "the data with appropriate label and standardizing/normalizing the data'''\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Encode the labels to {-1, 1}\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y) * 2 - 1\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "'''N = Number of qubits\n",
    "\n",
    " Here we have to make sure that the number of qubits are log2(n), where n = number of features.\n",
    "\n",
    " If we were using Angle embedding we would have used same number of qubits as number of features.'''\n",
    "N=5 \n",
    "\n",
    "if N**2 <= X_train.shape[1]:\n",
    "    num_features = N **2\n",
    "else:\n",
    "    num_features = X_train.shape[1]\n",
    "\n",
    "X_train=X_train[ : , :num_features]\n",
    "X_test=X_test[ : , : num_features]\n",
    "\n",
    "'''Creates quantum device that will run our circuits. Here we use 'default.qubit' Pennylane quantum circuit simulator'''\n",
    "# Define a device\n",
    "dev = qml.device(\"default.qubit\", wires=N)\n",
    "\n",
    "\n",
    "'''Define the quantum circuit.\n",
    "Here we define a parameterized quantum circuit (Ansatz). We use a simple variational circuit:\n",
    "'''\n",
    "@qml.qnode(dev)\n",
    "def variational_circuit(params, x):\n",
    "    qml.templates.AmplitudeEmbedding(x, wires=range(N), normalize=True, pad_with=0.0)\n",
    "    qml.templates.StronglyEntanglingLayers(params, wires=range(N))\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "\n",
    "def variational_classifier(params, bias, x):\n",
    "    return variational_circuit(params, x) + bias\n",
    "\n",
    "'''We need a cost function to optimize. This function calculates the loss based on \n",
    "the current circuit parameters and predictions. This create a loss function to be minimized'''\n",
    "# Define the cost function\n",
    "def cost(params, bias, X, y):\n",
    "    predictions = pnp.array([variational_classifier(params, bias, x) for x in X])\n",
    "    return pnp.mean((predictions - y) ** 2)\n",
    "\n",
    "'''Defining accuracy to evaluate performance'''\n",
    "#Define accuracy\n",
    "def accuracy(labels, predictions):\n",
    "    acc = sum(abs(l - p) < 1e-5 for l, p in zip(labels, predictions))\n",
    "    acc = acc / len(labels)\n",
    "    return acc\n",
    " \n",
    "'''Optimization step involves setting up the initial parameters.\n",
    "We then use classical optimizers to adjust the parameters of the quantum \n",
    "circuit to minimizet the cost function. \n",
    "We can use optimizers gradient descent optimizers or more advanced optimizers like \n",
    "Stochastic gradient descent (Adam) optimizer, NeterovMomentumOptimizer to update the hyperparameters\n",
    "'''\n",
    "# Initialize the parameters\n",
    "num_layers = 5\n",
    "params = pnp.random.uniform(low=0, high=2 * np.pi, size=(num_layers, N , 3), requires_grad=True)\n",
    "bias = pnp.array(0.0, requires_grad=True)\n",
    "# Set up the optimizer\n",
    "opt = NesterovMomentumOptimizer(0.3)\n",
    "epochs = 150\n",
    "\n",
    "'''We run optimization loop to update the hyperparameters'''\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    params, bias = opt.step(lambda p, b: cost(p, b, X_train, y_train), params, bias)\n",
    "    # params, bias = opt.step(cost, params, bias, X_train, y_train)\n",
    "    # print(params)\n",
    "    if (epoch + 1) % 5 == 0:            \n",
    "        current_cost = cost(params, bias, X_train, y_train)\n",
    "        print(f\"Epoch {epoch + 1}: Cost = {current_cost}\")\n",
    "\n",
    "'''Finally we use the predictions made using learned model to find the accuracy '''\n",
    "# Test the trained model\n",
    "predictions = pnp.array([np.sign(variational_classifier(params, bias, x)) for x in X_test])\n",
    "accuracy = pnp.mean(predictions == y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "# acc.append([layers, step, round(accuracy.item(), 4)])\n",
    "# print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
